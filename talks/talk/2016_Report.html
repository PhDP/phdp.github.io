<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Transfer &amp; Statistical Relational Learning with Applications to Biodiversity</title>

    <meta name="description" content="Markov Logic: Logic, Probability, Learning">
    <meta name="author" content="Philippe Desjardins-Proulx">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/gryphon.css" id="theme">
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <script>
      if (window.location.search.match(/print-pdf/gi)) {
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'css/print/pdf.css';
        document.getElementsByTagName('head')[0].appendChild(link);
      }
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">
      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Transfer &amp; Statistical Relational Learning with Applications to Biodiversity</h1>
          <p>by <a href="http://phdp.github.io/">Philippe Desjardins-Proulx</a></p>
          <p><a href="http://phdp.github.io/">Université de Sherbrooke</a></p>
        </section>

        <section>
          <section>
            <h1>Content</h1>
            <ol style='list-style-type: upper-roman'>
              <li>Machine Learning</li>
              <li>Paper 0: Ecological Interactions and the Netflix Problem</li>
              <li>Paper 1: Transfer from Ecological Theory</li>
              <li>Paper 4: Logic-Based Machine Learning</li>
              <li>Manticore</li>
              <li>Paper 2: Hypergraph Lifting on GPUs</li>
              <li>Paper 3: Indirect Genotype-Phenotype mapping</li>
            </ol>
          </section>
        </section>

        <section>
          <section>
            <h1>Machine Learning</h1>
          </section>
          <section>
            <h2>Three paradigms of machine learning</h2>
            <p><b>Supervised learning</b></p>
            \[\{(\mathbf{x}, y)_i\}_{i=0}^{n-1} \mapsto (\hat{f}(\mathbf{x}) \mapsto y).\]
            <br/>
            <p><b>Unsupervised learning</b></p>
            \[\{(\mathbf{x})_i\}_{i=0}^{n-1} \mapsto (\hat{f}(\mathbf{x})).\]
            <br/>
            <p><b>Reinforcement learning</b></p>
            \[\langle S, A, T, R \rangle.\]
            <br/>
            <h2>Mapping ML concepts to statistics</h2>
            <p>Supervised Learning \(\approx\) Classification, Regression.</p>
            <p>Unsupervised Learning \(\approx\) Clustering.</p>
            <p>Reinforcement learning and non-statistical learning (e.g. inductive logic programming) have no equivalents in statistics.</p>
          </section>
          <section>
            <h2>Unsupervised Learning: Matrix completion</h2>
            <p>Filling missing entries or (positive-only case) make recommendations.</p>
            <p>K-nearest neighbors (instance learning, paper 0)</p>
            <p>Deep neural networks (paper 1)</p>
            <p><a href='https://devblogs.nvidia.com/parallelforall/accelerate-recommender-systems-with-gpus/'>Matrix factorization</a> (Alternating Least Square)</p>
            <p><img src='img/als.png' alt='als'/></p>
          </section>
          <section>
            <h2>Unsupervised Learning: Probabilistic Graphical Models</h2>
            <p>\(P(A|B)\) is a whole different thing with many variables...</p>
            <p><a href='http://logic.pdmi.ras.ru/~sergey/papers/TN05_DirectedCycles.pdf'>AL Tulupyev and SI Nikolenko. Directed Cycles in Bayesian Belief Networks: Probabilistic Semantics and Consistency Checking Complexity, 2005.</a></p>
          </section>
          <section>
            <h2>Holy Quadrality</h2>
            <ol style='list-style-type: upper-roman'>
              <li>Inference: Conditional/Joint/Marginal.</li>
              <li>Inference: MAP.</li>
              <li>Learning: Weights.</li>
              <li>Learning: Structure.</li>
            </ol>
            <br/><br/>
            <p>True for all PGMs and many similar knowledge bases: Bayesian networks, Markov fields, Markov logic, ...</p>
          </section>
          <section>
            <h2>Inference: Conditional/Joint/Marginal</h2>
            <p>Solve \(P(\mathbf{X} = \mathbf{x} \mid \mathbf{Y} = \mathbf{y})\), or just \(P(\mathbf{X} = \mathbf{x})\), or just \(P(x = v)\).</p>
            <p><img src='img/markov_net_evidence.svg' alt='Markov network'/></p>
            \[P(a = 1 \mid b = 1, e = 1, f = 0, g = 0) = 0.145.\]
          </section>
          <section>
            <h2>Inference: MAP</h2>
            <p>Find the most likely joint state of a subset \(\mathbf{X'} \subset \mathbf{X}\).
            <p><img src='img/markov_net_evidence.svg' alt='Markov network'/></p>
            \[MAP(b = 1, e = 1, f = 0, g = 0) = \{a, b, d, e, h = 1, d, f, g = 0\}.\]
          </section>
          <section>
            <h2>Learning: Weight</h2>
            <p>Learning from data the strenght of the relationship between variables.</p>
            <p>Example: learning \(w\) in the previous example.</p>
          </section>
          <section>
            <h2>Learning: Structure</h2>
            <p>Learning which vertices are connected by an edge.</p>
          </section>
          <section>
            <h2>Interlude: Anatomy of Markov networks</h2>
            <p>...or Markov fields, undirected graphical models.</p>
            <p><img src='img/markov_net.svg' alt='Markov network'/></p>
            <p>A Markov Network with 8 variables, 9 edges, and 3 factors:</p>
            \[P(a, b, c, d, e, f, g) = \frac{1}{Z}\phi_0(a, b, c)\phi_1(d)\phi_2(e, f, g, h).\]
          </section>
          <section>
            <p>Markov network are often used as log-linear models:</p>
            <p>\[P(X = x) = \frac{1}{Z}\exp\left(\sum_j w_j f_j(x)\right).\]
            <p>With \(Z\) being a factor for normalization.</p>
            <p>There is no closed form solution for maximum likelihood or
            maximum a posteriori probability, but since the function is
            concave, it is fairly easy to compute with gradient methods.</p>
            <p>Based on MaxEnt.</p>
          </section>
        </section>

        <section>
          <section>
            <h1>Paper 0: Ecological Interactions and the Netflix Problem</h1>
            <!-- I curse all food webs, and for entire field of interaction ecology, for submitting without a fight to the k nearest neighbors. -->
          </section>
          <section>
            <h2>Data</h2>
            <p>881 species.</p>
            <p>~4% of the entries in the 881 by 881 interaction matrix are interactions.</p>
            <p>24 binary traits + body mass for each species.</p>
          </section>
          <section>
            <h2>K nearest neighbour for interactions</h2>
            <p><img src='img/pearson.svg' alt='pearson'/></p>
          </section>
          <section>
            <h2>Tanimoto similarity</h2>
<pre><code class="java">public double userSimilarityForItems(long userID1, long userID2) throws TasteException { /* ... */ }

public double userSimilarityForTraits(long x, long y) {
  HashSet&lt;String&gt; xTraits = userTraits.get(x);;
  HashSet&lt;String&gt; yTraits = userTraits.get(y);

  // Compute tanimoto on items:
  int xTraitsSize = xTraits.size();
  int yTraitsSize = yTraits.size();
  if (xTraitsSize == 0 || yTraitsSize == 0) {
    return 0.0;
  }

  int intersectionSize =
      xTraitsSize < yTraitsSize ? setIntersectionSize(yTraits, xTraits) : setIntersectionSize(xTraits, yTraits);

  int unionSize = xTraitsSize + yTraitsSize - intersectionSize;
  return (double) intersectionSize / (double) unionSize;
}

@Override
public double userSimilarity(long x, long y) throws TasteException {
  final double byItems = userSimilarityForItems(x, y);
  final double byTraits = userSimilarityForTraits(x, y);
  return (1.0 - weightToTraits) * byItems + weightToTraits * byTraits;
}
</code></pre>
          </section>
          <section>
            <h2>Results</h2>
            <p><img src='img/tanimoto.png' alt='tanimoto results'/></p>
          </section>
          <section>
            <h2>Results</h2>
            <p>Supervised learning fails to separate interactions from non-interactions (98% of the entries in the non-interactions are found in the interactions data-set).</p>
            <p>Tanimoto (positive-only) without using traits is the most effective method.</p>
          </section>
        </section>

        <section>
          <section>
            <h1>Paper 1: Transfer from Ecological Theory</h1>
          </section>
          <section>
            <h2>Deep Learning Representation</h2>
            <p><img width='80%' src='img/deep.png' alt='deep'/></p>
          </section>
          <section>
            <h2>tl;dr</h2>
            <ul>
              <li>Train deep networks with data generated with the niche model.</li>
              <li><a href='https://github.com/PhDP/WebGen'>C++11 code to generate webs</a>.</li>
              <li>Python script (Theano) for deep learning (Boltzmann machines).</li>
              <li>Write the niche model in first-order logic?</li>
            </ul>
          </section>
        </section>

        <section>
          <section>
            <h1>Paper 4: Logic-Based Machine Learning</h1>
          </section>
          <section>
            <table>
              <tr>
                <th>Language</th>
                <th>Ontological commitment</th>
                <th>Epistemological commitment</th>
              </tr>
              <tr>
                <td>First-Order Logic</td>
                <td>Facts, objects, relations</td>
                <td>True | False | Unknown</td>
              </tr>
              <tr>
                <td>Probability Theory</td>
                <td>Facts</td>
                <td>Degree of belief \(\in [0, 1]\)</td>
              </tr>
              <tr>
                <td>Markov Logic</td>
                <td>Facts, objects, relations</td>
                <td>Degree of belief \(\in [0, 1]\)</td>
              </tr>
            </table>
            <p><i>Adapted from <a href='http://aima.cs.berkeley.edu/'>Russell &amp; Norvig</a> (3rd ed., p 290).</i></p>
          </section>
          <section>
            <h2>In a nutshell</h2>
            <ol style='list-style-type: upper-roman'>
              <li>...Markov logic is about learning first-order logic sentences, each assigned with a probability \(\in [0, 1]\).</li>
              <li>Probability theory offers soft constraints, a world where many sentences are proven wrong is not impossible, but less
              likely.</li>
              <li>The more confident we are in a sentence contradicted by some evidence, the less likely the world is.</li>
              <li>Markov logic is a stict superset of first-order logic since \(p = 1.0\) is equivalent to <b>true</b>, and \(p = 0.0\) is equivalent to <b>false</b>.</li>
              <li>And it also a subperset of discrete and finite-precision probability distributions. That is: it has been proven that every probability distribution over discrete or finite-precision numeric variables can be
represented as a Markov logic network.</li>
            </ol>
          </section>
          <section>
            <h2>The case for Markov Logic</h2>
            <ol style='list-style-type: upper-roman'>
              <li>Logic for complexity. Probability theory for uncertainty (<a href='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.9497&rep=rep1&type=pdf'>e.g. collective classification of links</a>).</li>
              <li>Not a black box: FOL sentences are easy to understand (<a href='http://www.biomedcentral.com/1471-2105/14/273'>example</a>).</li>
              <li>Facilitates human interventions (knowledge engineering).</li>
              <li>Facilitates computer interventions (e.g. Mihalkova's transfer algorithm).</li>
            </ol>
          </section>
          <section>
            <h2>First-Order Logic</h2>
            <h2>How expressive / powerful is first-order logic?</h2>
            <p>The general consensus is that it's powerful enough to formalize much of modern mathematics, but not enough for some nuances of natural languages.</p>
            <p>Bottom line: it's <b>very</b> expressive, and since it's enough for modern mathematics, it's probably enough for most (or all) knowledge engineering.</p>
          </section>
          <section>
            <h2>Terms</h2>
            <p><i>...refer to objects, which can be anything from integers to species, cities, words...</i></p>
            <ol style='list-style-type: upper-roman'>
              <li><b>Variables</b>, e.g. \(x, y, z\). The convention is to start variable names with a lowercase character. A variable ranges over objects of a certain type.</p>
              <li><b>Constants</b>, e.g \(Wolf, 0, Tokyo\), represent actual objects in the domain.</li>
              <li><b>Functions</b>, which are mappings between 0 o more terms and another term.</li>
            </ol>
          </section>
          <section>
            <h2>Atomic sentences</h2>
            <p><i>An atomic sentence is an element that, alone, is a valid first-order logic sentence.</i></p>
            <ol style='list-style-type: upper-roman'>
              <li><b>True</b> or <i>top</i>: \(\top\), and it's negation <b>False</b> or <i>bottom</i>: \(\bot\).</li>
              <li><b>Predicates</b>, which are mappings between 0 or more <b>terms</b> to a truth value. They have the same form as functions but can be distinguished by the context: predicates are atomic sentences, not functions.</li>
              <li><b>Identity</b>, represented by the mysterious = symbol, tests if two terms are the same. Identity can be (and often is) seen as a predicate with two arguments.</li>
            </ol>
          </section>
          <section>
            <h2>A few sentences</h2>
            <p style='color:#10CC10'>\[PrimeNumber(11)\]</p>
            <p style='color:#10CC10'>\[PrimeNumber(Minus(8, 1))\]</p>
            <p style='color:#10CC10'>\[\top\]</p>
            <p style='color:#10CC10'>\[FatherOf(Jesus) = God\]</p>
            <p style='color:#10CC10'>\[Identity(FatherOf(Jesus), God)\]</p>
            <p style='color:#C24617'>\[Dist(Helsinki, Jerusalem)\]</p>
            <p style='color:#10CC10'>\[Dist(Helsinki, Jerusalem) = 7653.8km\]</p>
            <p style='color:#10CC10'>\[GreaterThan(Dist(Helsinki, Jerusalem), Dist(Montreal, Quebec))\]</p>
            <p style='color:#10CC10'>\[Dist(Helsinki, Jerusalem) > Dist(Montreal, Quebec)\]</p>
          </section>
          <section>
            <h2>Connectives</h2>
            <p><i>...connect sentences</i></p>
            <ol style='list-style-type: upper-roman'>
              <li>The binary connective <b>and</b>: \(x \land y\), which is true only if both \(x\) and \(y\) are true. Like all other connective shown here, if \(x\) and \(y\) are sentences, then \(x \land y\) is also a valid sentence.</li>
              <li>The binary connective <b>or</b>: \(x \lor y\), which is true only if \(x\) is true, if \(y\) is true, or if both are true.</li>
              <li>The binary connective <b>implies</b>: \(x \implies y\), returns true in all cases, except if \(x\) is true and \(y\) is false.</li>
              <li>The binary connective <b>iff</b>: \(x \leftrightarrow y\), returns true if \(x\) and \(y\) have the same value, that is if they are both true, or both false.</li>
              <li>The binary connective <b>xor</b> (exclusive or): \(x \oplus y\), returns true if \(x\) and \(y\) have different values.</li>
              <li>The unary connective <b>not</b>: \(\lnot x\), which is true only if \(x\) is false.</li>
              <li>The qualifiers <b>for all</b> and <b>exists</b> (respectively \(\forall\) and \(\exists\)).</li>
            </ol>
          </section>
          <section>
            <h2>Grounding</h2>
            <p><i>We talk of a ground term or ground sentence if no variables are present.</i></p>
            <p style='color:#C24617'>\[\forall y \exists x, y \neq White \implies BrighterThan(x, y)\]</p>
            <p style='color:#10CC10'>\[BrighterThan(Pink, Red)\]</p>
          </section>
          <section>
            <h2>Second and Higher-Order Logic</h2>
            <p style='color:#10CC10'>\[\forall n\ GermainPrime(n) \implies PrimeNumber(2p + 1)\]</p>
            <p style='color:#10CC10'>\[\forall n\ GermainPrime(n) \implies PrimeNumber(Add(Multiply(2, p), 1))\]</p>
            <p style='color:#10CC10'>\[\forall x\ Pierre(x) \land Roule(x) \implies \lnot AmasseMousse(x)\]</p>
            <p>Santa Claus is a sadist</p>
            <p style='color:#10CC10'>\[Sadist(SantaClaus)\]</p>
            <p>Santa Claus has all the attributes of a sadist</p>
            <p style='color:#C24617'>\[\forall foo(Sadist)\ foo(SantaClaus)\]</p>
          </section>
          <section>
            <h2>Example 0: A bit of arithmetics</h2>
            <ol style='list-style-type: upper-roman'>
              <li>Let's define the predicates GreaterThan, SmallerThan, Equal, each taking two real numbers.</li>
              <li>Then, we can define the functions Addition, Multiplication, each taking two arguments (real numbers) and returning a real number.</li>
            </ol>
            <p style='color:#10CC10'>\[42 > 2 \times 6\]</p>
            <p style='color:#10CC10'>\[GreaterThan(Addition(40, 2), Multiply(2, 6))\]</p>
            <p style='color:#10CC10'>\[\forall x\ GreaterThan(1, Multiply(0, x))\]</p>
            <p style='color:#10CC10'>\[\forall x, y\ Equals(Addition(x, y), Addition(y, x))\]</p>
            <p style='color:#10CC10'>\[\lnot \exists x, y, z\ Equals(z, Addition(x, y)) \land GreaterThan(z, Addition(x, y)) \]</p>
            <p style='color:#10CC10'>\[GreaterThan(2, 6)\]</p>
            <p style='color:#C24617'>\[Multiply(2, 6)\]</p>
          </section>
          <section>
            <h2>Example 1: Peano axioms</h2>
            <p>\[NatNum(0)\]</p>
            <p>\[\forall n\ NatNum(n) \implies NatNum(S(n))\]</p>
            <p>\[\forall n\ 0 \not= S(n)\]</p>
            <p>\[\forall m, n\ m \not = n \implies S(m) \not = S(n)\]</p>
            <p>\[\forall m\ NatNum(m) \implies Addition(0, m) = m\]</p>
            <p>\[\forall m, n\ NatNum(m) \land NatNum(n) ⇒ Addition(S(m), n) = S(Addition(m, n))\]</p>
          </section>
          <section>
            <h2>Knowledge engineering</h2>
            <p>Knowledge engineering is the task of establishing the rules for some system. It is common in engineering, business
            intelligence, health informatics, chemo-informatics, logisic problems, etc etc. Normally, it is done by deciding on a
            set of first-order sentences.</p>
          </section>
          <section>
            <h2>Example 2: Ecology?</h2>
            <p>\[s0: Presence(GreyWolf)\]</p>
            <p>\[s1: \forall x,y,z\ PreyOn(x, z) \land PreyOn(y, z) \implies Compete(x, y)\]</p>
            <p>\[s2: \forall x,y\ PreyOn(x, y) \implies Larger(x, y) \lor Parasite(x)\]</p>
            <p>\[s3: \forall x,y\ SameNiche(x, y) \implies \lnot CoOccur(x, y)\]</p>
            <p>\[s4: \exists x\ NumPreys(x) = 0\]</p>
            <p>\[s5: \forall x,y,z\ NumPreys(x) = 1 \land NumPreys(y) = 1 \land PreyOn(x, z) \land\]</p>
            <p>\[PreyOn(y, z) \implies \lnot CoOccur(x, y)\]</p>
          </section>
          <section>
            <h2>A Few Gotchas</h2>
            <p>Implication works well with the \(\forall\) qualifier:</p>
            <p>\[\forall x,y,z\ PreyOn(x, z) \land PreyOn(y, z) \implies Compete(x, y)\]</p>
            <p>\[\forall x,y,z\ PreyOn(x, z) \land PreyOn(y, z) \land Compete(x, y)\]</p>
            <p>...but be careful when the left side is rarely true (solution: types), it messes up probabilistic inference. Think of the above sentences if x, y, z ranges over all objects (cities, people, species) vs only species.</p>
          </section>
          <section>
            <h2>Qualification order</h2>
            <p>"Everybody loves somebody":</p>
            <p>\[\forall x \exists y\ Loves(x, y)\]</p>
            <p>"There is someone who is loved by everyone":</p>
            <p>\[\exists y \forall x\ Loves(x, y)\]</p>
            <p>The qualfications should be read:</p>
            <p>\[\forall x\ (\exists y\ Loves(x, y))\]</p>
          </section>
          <section>
            <h2>Order of Precedence</h2>
            <p>...differs from author to author.</p>
            <p>A common scheme: \[\lnot, =, \land, \lor, \implies, \oplus, \iff\].</p>
          </section>
          <section>
<div class='terminal'><pre>$ cat data/Ecology.txt
Forall x,y,z  PreyOn(x, z) and PreyOn(y, z) => Compete(x, y)
Forall x,y    PreyOn(x, y) => Larger(x, y) or Parasite(x)
Forall x,y    SameNiche(x, y) => !CoOccur(x, y)
Exists x      NumPreys(x) = 0</pre></div>
          </section>
          <section>
            <h2>Markov Logic Networks</h2>
            <h2>The Gist of It</h2>
            <p>A Markov Logic Network \(L\) is a set of tuples made of first-order logic sentences and weights: \(L = (s_0, w_0), (s_1, w_1), (s_2, w_2), ...\)</p>
<div class='terminal'><pre>0.9 Forall x,y,z  PreyOn(x, z) and PreyOn(y, z) => Compete(x, y)
1.4 Forall x,y    PreyOn(x, y) => Larger(x, y) or Parasite(x)
1.1 Forall x,y    SameNiche(x, y) => !CoOccur(x, y)
0.8 Exists x      NumPreys(x) = 0
</pre></div>
          </section>
          <section>
            <h2>Where's my Network?</h2>
            <p>Together with a finite set of constants \(C = c_0, c_1, ...\), the Markov logic network \(L\) defines
            a Markov network \(M_{L,C}\) as follow:</p>
            <ol style='list-style-type: upper-roman'>
              <li>\(M_{L, C}\) contains one binary vertex for each possible grounding of each predicate appearing in \(L\).
The value of the vertex is 1 if the ground predicate is true, and 0 otherwise.</li>
              <li>\(M_{L, C}\) contains one feature (vertex of the factor graph) for each possible grounding of each sentence \(s_i \in L\). The value of
this feature is 1 if the ground formula is true, and 0 otherwise. The weight of the feature is the
weight associated with \(s_i\) in \(L\).</li>
            </ol>
          </section>
          <section>
            <h2>The facts</h2>
<div class='terminal'><pre>Friends(Anna, Bob)
Friends(Anna, Edward)
Friends(Anna, Frank)
Friends(Edward, Frank)
Friends(Gary, Helen)
!Friends(Gary, Frank)
Smokes(Anna)
Smokes(Edward)</pre></div>
          </section>
          <section>
            \[s_0: \forall x\ Smoking(x) \implies Cancer(x)\]
            \[s_1: \forall x, y\ Friend(x, y) \land Smoking(x) \implies Smoking(y)\]
            \[L: (s_0, 1.5), (s_1, 1.1)\]
            \[C: Anna, Bob\]
            <p><img src='img/mln.png' alt='Markov logic network'/></p>
            <p>Lots of possible things to do, such as updating the probabilities with the formula, evaluating \(P(Cancer(x))\), ...</p>
          </section>
        </section>

        <section>
          <section>
            <h1>Manticore</h1>
            <ul>
              <li>Pure C++11/Cuda header-only library (might have a Python package).</li>
              <li>MIT-licensed.</li>
              <li>Focuses on GPU (CUDA8) computing for statistical relational learning.</li>
              <li>For GPUs: supports Linux (gcc) and Windows (msvc 2015)</li>
              <li>For CPUs: supports Linux (gcc, clang, intel), Windows (msvc 2015), probably OSX.</li>
              <li>Good support for logic, basic structures for CAS.</li>
            </ul>
          </section>
          <section>
            <h1>Manticore</h1>
            <ul>
              <li>Logic: transformation to clauses, simplification, evaluation, disjunctive normal form, conjunctive normal form, SAT.</li>
              <li>Markov logic: conditional inference: brute force only (CPU).</li>
              <li>Markov logic: MAP inference: MaxWalkSat (CPU), Plane-Cutting (GPU).</li>
              <li>Markov logic: structure learning: Kok, Hypergraph lifting (CPU, GPU).</li>
              <li>Markov logic: weight learning (exact same code as Alchemy).</li>
            </ul>
          </section>
          <section>
<pre><code class="cpp">using formula = boost::variant&lt;
  top,
  bottom,
  predicate,
  boost::recursive_wrapper&lt;negation&gt;,
  boost::recursive_wrapper&lt;conjunction&gt;,
  boost::recursive_wrapper&lt;disjunction&gt;,
  boost::recursive_wrapper&lt;ex_disjunction&gt;,
  boost::recursive_wrapper&lt;implication&gt;,
  boost::recursive_wrapper&lt;equivalence&gt;,
  boost::recursive_wrapper&lt;forall&gt;,
  boost::recursive_wrapper&lt;exists&gt;&gt;;</code></pre>
          </section>
          <section>
<pre><code class="cpp">
using term = boost::variant&lt;
  std::string,                          // Variable
  int64_t,                              // \
  double,                               //  |
  vector&lt;double&gt;,                       //  | Constants
  matrix&lt;double&gt;,                       //  |
  boost::container::flat_set&lt;int64_t&gt;,  //  |
  boost::container::flat_set&lt;double&gt;,   // /
  boost::recursive_wrapper&lt;function&gt;;   // Function</code></pre>
          </section>
          <section>
<div class='terminal'><pre> from sympy import *
>>> x = Symbol('x')
>>> simplify(sqrt(x**2))
sqrt(x**2)
</pre></div>
          </section>
          <section>
            \[\forall x: Number(x) \implies (NonNeg(x) \oplus Neg(x)),\]
            \[\forall x: NonNeg(x) \implies Simplify(\sqrt{x^2}) = x,\]
            \[\forall x: Neg(x) \implies Simplify(\sqrt{x^2}) = -x.\]
          </section>
          <section>
<pre><code class="cpp">template&lt;typename T = std::string&gt;
class clause {
public:
  typedef boost::container::flat_set&lt;T&gt; set_type;

private:
  set_type m_ps; // Positive literals.
  set_type m_ns; // Negative literals.</code></pre>
          </section>
          <section>
            <h2>Logic is a solid foundation for doing maths</h2>
            <p>Simplify</p>
            \[\sqrt{x^2}.\]
          </section>
        </section>

        <section>
          <section>
            <h1>Paper 2: Hypergraph Lifting on GPUs</h1>
          </section>
          <section>
            <p>Gene regulatory network inference with Markov Logic (<a href='http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-273'>Brouard et al., 2013</a>).</p>
            <p><img src='img/brouard2013.svg' alt='Brouard'/></p>
            <ul>
              <li><b>Aleph</b>: Determinisitic ILP (Inductive Logic Programming) engine, CPU.</li>
              <li><b>Alchemy</b>: Probabilistic engine (CPU).</li>
              <li><b>Manticore</b>: Probabilistic engine (CPU + GPU).</li>
            </ul>
          </section>
          <section>
            <h2>Learning new formulas</h2>
            <p>Check \(Foo(x, y)\).</p>
            <p>Check \(\neg Foo(x, y)\).</p>
            <p>Check \(Bar(x, y, z)\).</p>
            <p>Check \(\neg Foo(x, y)\).</p>
            <p>Check \(Foo(x, y) \land Bar(x, y, z)\).</p>
            <p>Check \(\neg Foo(x, y) \land Bar(x, y, z)\).</p>
            <p>...</p>
          </section>
          <section>
            <h2>Bottom-Up Strategies</h2>
            <p>Why not only check formulas that are at least true once in our data?</p>
          </section>
          <section>
            <h2>Relational Data as a Hypergraph</h2>
            <p>Three constants \(\{Elaine, George, Jerry\}\) and two formulas:</p>
            <p>\(Friend(x, y) \land Smoking(x) => Smoking(y)\),</p>
            <p>\(Smoking(x) => Cancer(x)\).</p>
            <p><img src='img/ground_seinfeld.svg' alt='seinfeld ground'/></p>
            <p>With real data-set (thousands of constants, dozens of formulas), the networks blows up to billions of nodes.</p>
          </section>
          <section>
            <h2>Behold the Hypergraph!</h2>
            <p><img src='img/hypergraph.svg' alt='hypergraph'/></p>
          </section>
          <section>
            <h2>Strategies</h2>
            <p>Search on CPUs (parallel), compute the WPLL of the candidates (weighted pseudo-log-likelihood) on GPUs.</p>
            <p>Do both on GPUs with CUDA 8 graph analytics library.</p>
          </section>
        </section>

        <section>
          <section>
            <h1>Paper 3: Indirect Genotype-Phenotype mapping</h1>
          </section>
          <section>
            <h2>Direct Approach</h2>
            <p><img width='40%' src='img/DNA-to-Phenotypes-Direct.png' alt='direct'/></p>
          </section>
          <section>
            <h2>Indirect Approach</h2>
            <p><img width='50%' src='img/DNA-to-Phenotypes-Cell-Variables.png' alt='direct'/></p>
          </section>
          <section>
            <h2>Plan</h2>
            <ol style='list-style-type: upper-roman'>
              <li><a href='http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3122930/'>Markov Logic Networks in the Analysis of Genetic Data</a>.</li>
              <li>Data-set \(\alpha\): Xiong et al. <a href='http://sites.utoronto.ca/intron/xiong2015.pdf'>The human splicing code reveals new insights into the genetic determinants of disease</a>.</li>
              <li>Data-set \(\beta\): Zheng and Dicke. <a href='http://www.plantphysiol.org/content/146/3/812.full'>Ecological Genomics of Plant-Insect Interactions: From Gene to Community</a>.</li>
            </ol>
          </section>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    <script>
      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none

        // Parallax scrolling
        // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
        // parallaxBackgroundSize: '2100px 900px',

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
      });
    </script>
    <script type="text/javascript" src="../js/bower/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </body>
</html>
